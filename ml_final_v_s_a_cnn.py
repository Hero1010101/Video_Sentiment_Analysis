# -*- coding: utf-8 -*-
"""ML_Final_V_S_A_CNN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11ZuPCUuDHIb33CzodEGRNIpNtbMlLTZh
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms

# Define a simple CNN architecture
class CNNModel(nn.Module):
    def __init__(self):
        super(CNNModel, self).__init__()
        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)  # Input channels: 3 (RGB), Output channels: 32
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)  # Output channels: 64
        self.pool = nn.MaxPool2d(2, 2)  # Max-pooling layer
        self.fc1 = nn.Linear(64 * 8 * 8, 512)  # Fully connected layer
        self.fc2 = nn.Linear(512, 10)  # Output layer for 10 classes (CIFAR-10)
        self.relu = nn.ReLU()  # ReLU activation function
        self.dropout = nn.Dropout(0.5)  # Dropout for regularization

    def forward(self, x):
        x = self.pool(self.relu(self.conv1(x)))  # Convolution + ReLU + Pooling
        x = self.pool(self.relu(self.conv2(x)))  # Convolution + ReLU + Pooling
        x = x.view(-1, 64 * 8 * 8)  # Flatten the tensor for fully connected layers
        x = self.relu(self.fc1(x))  # Fully connected + ReLU
        x = self.dropout(x)  # Apply dropout
        x = self.fc2(x)  # Output layer
        return x

# Transformations for the data augmentation
transform = transforms.Compose([
    transforms.Resize((32, 32)),  # CIFAR-10 image size
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

# Load CIFAR-10 dataset
train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)

# Initialize the custom CNN model
cnn_model = CNNModel()

# Select device for training (GPU if available, else CPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
cnn_model = cnn_model.to(device)

# Loss and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(cnn_model.parameters(), lr=0.001)

# Training loop (same as before)
def train_model(model, train_loader, criterion, optimizer, device):
    model.train()
    for epoch in range(10):  # Run for 10 epochs
        running_loss = 0.0
        for i, (inputs, labels) in enumerate(train_loader):
            inputs, labels = inputs.to(device), labels.to(device)

            # Zero the gradients
            optimizer.zero_grad()

            # Forward pass
            outputs = model(inputs)
            loss = criterion(outputs, labels)

            # Backward pass
            loss.backward()
            optimizer.step()

            running_loss += loss.item()
            if i % 100 == 99:  # Print every 100 batches
                print(f'Epoch [{epoch+1}], Step [{i+1}], Loss: {running_loss / 100:.4f}')
                running_loss = 0.0

# Test loop (same as before)
def test_model(model, test_loader, device):
    model.eval()  # Set the model to evaluation mode
    correct = 0
    total = 0
    with torch.no_grad():
        for inputs, labels in test_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    print(f'Test Accuracy: {100 * correct / total}%')

# Train and test the CNN model
print("Training CNN Model...")
train_model(cnn_model, train_loader, criterion, optimizer, device)
test_model(cnn_model, test_loader, device)

# Save the trained CNN model
torch.save(cnn_model.state_dict(), 'cnn_model.pth')

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import transforms
import cv2
from collections import Counter
from PIL import Image

# Define a simple CNN architecture for Facial Expression Recognition
class CNNModel(nn.Module):
    def __init__(self):
        super(CNNModel, self).__init__()
        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)  # Input channels: 3 (RGB), Output channels: 32
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)  # Output channels: 64
        self.pool = nn.MaxPool2d(2, 2)  # Max-pooling layer
        self.fc1 = nn.Linear(64 * 56 * 56, 512)  # Adjust for 224x224 input size
        self.fc2 = nn.Linear(512, 8)  # Output layer for 8 emotion classes
        self.relu = nn.ReLU()  # ReLU activation function
        self.dropout = nn.Dropout(0.5)  # Dropout for regularization

    def forward(self, x):
        x = self.pool(self.relu(self.conv1(x)))  # Convolution + ReLU + Pooling
        x = self.pool(self.relu(self.conv2(x)))  # Convolution + ReLU + Pooling
        x = x.view(-1, 64 * 56 * 56)  # Flatten the tensor for fully connected layers
        x = self.relu(self.fc1(x))  # Fully connected + ReLU
        x = self.dropout(x)  # Apply dropout
        x = self.fc2(x)  # Output layer
        return x

# Enhanced Pretrained Model for Facial Expression Recognition (FER) using CNN
class FacialExpressionModel(nn.Module):
    def __init__(self):
        super(FacialExpressionModel, self).__init__()
        self.cnn = CNNModel()  # Use custom CNN
        self.softmax = nn.Softmax(dim=1)

    def forward(self, x):
        x = self.cnn(x)
        x = self.softmax(x)  # Get probabilities for each class
        return x

# Loads and preprocesses video for FER with data augmentation
def process_video(video_path):
    cap = cv2.VideoCapture(video_path)
    transform = transforms.Compose([
        transforms.ToPILImage(),
        transforms.Resize((224, 224)),
        transforms.RandomHorizontalFlip(),  # Augmentation: flips frames horizontally
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break
        frame_tensor = transform(frame)
        yield frame_tensor.unsqueeze(0)  # Add batch dimension

    cap.release()

# Video Sentiment Analysis model using the CNN-based face model
class VideoSentimentAnalysisModel(nn.Module):
    def __init__(self):
        super(VideoSentimentAnalysisModel, self).__init__()
        self.face_model = FacialExpressionModel()

    def forward(self, face_input):
        face_output = self.face_model(face_input)
        return face_output

# Initialize the Video Sentiment Analysis model
video_sentiment_model = VideoSentimentAnalysisModel()

# Set model to evaluation mode and disable gradient computation
video_sentiment_model.eval()

# Video path
video_path = "/content/smile.mp4"

# Process video frames
video_frames = process_video(video_path)

# Label emotions
emotion_labels = ['Happy', 'Sad', 'Angry', 'Scared', 'Confused', 'Neutral', 'Stressed', 'Disgust']

# List to store predictions
predictions = []

# Make predictions on each frame
with torch.no_grad():
    for frame_tensor in video_frames:
        sentiment = video_sentiment_model(frame_tensor)
        predicted_emotion = torch.argmax(sentiment, dim=1)
        predictions.append(emotion_labels[predicted_emotion.item()])

# Count occurrence of each emotion
emotion_counts = Counter(predictions)

# Get final predicted emotion based on the highest count
final_predicted_emotion = emotion_counts.most_common(1)[0][0]

# Calculate accuracy for each emotion
total_frames = len(predictions)
accuracy = {emotion: (count / total_frames) * 100 for emotion, count in emotion_counts.items()}

# Print results
print(f"Final predicted emotion: {final_predicted_emotion}")
print("Emotion accuracies:")
for emotion, acc in accuracy.items():
    print(f"{emotion}: {acc:.2f}%")